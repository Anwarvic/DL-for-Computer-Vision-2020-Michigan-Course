# Assignment 1
In this assignment, you will first learn how to use PyTorch on Google Colab environment. You will then practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor. The goals of this assignment are as follows:

- Develop proficiency with PyTorch tensors
- Gain experience using notebooks on Google Colab
- Understand the basic Image Classification pipeline and the - data-driven approach (train/predict stages)
- Understand the train/val/test splits and the use of validation - data for hyperparameter tuning
- Implement and apply a k-Nearest Neighbor (kNN) classifier
- Learn how to test your implementation on Autograder

## Q1: PyTorch 101 (60 points)
The notebook `pytorch101.ipynb` will walk you through the basics of working with tensors in PyTorch. You are required to write code on `pytorch101.py`.

## Q2: k-Nearest Neighbor classifier (40 points)
The notebook `knn.ipynb` will walk you through implementing a kNN classifier. Your implementation will go to `knn.py`.

## Steps
### 1. Download the zipped assignment file
Click [here](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/assignments/A1.zip) to download the starter code

### 2. Unzip all and open the Colab file from the Drive
Once you unzip the downloaded content, please upload the folder to your Google Drive. Then, open each `*.ipynb` notebook file with Google Colab by right-clicking the `*.ipynb` file. No installation or setup is required! For more information on using Colab, please see our [Colab tutorial](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/colab.html).

### 3. Open your corresponding `*.py` from Google Colab and work on the assignment
Next, we recommend editing your `*.py` file on Google Colab, set the ipython notebook and the code side by side. Work through the notebook, executing cells and implementing the codes in the `*.py` file as indicated. You can save your work, both `*.ipynb` and `*.py`, in Google Drive (click “File” -> “Save”) and resume later if you don’t want to complete it all at once.

